# Collection of stuff for MP presentation...


### interesting points

* interpreting embeddings and understanding the encoded grammatical and semantic relations between words is useful, but challenging
* Context can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.
* 


### Reading

* [Skip gram explained](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* 